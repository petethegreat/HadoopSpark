---
title: "Pyspark ML pipeline"
output:
  html_document:
    theme: "cerulean" 
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  pdf_document:
    toc: true
---


## Introduction
This summarises some work I did using PySpark's machine learning (ML) library to predict survival for the Kaggle Titanic dataset. The data is loaded from a .csv file on hdfs into a spark dataframe, and then run through a spark pipeline that transforms the data and fits a logistic regression model. The output model is then applied to the test set. The Python file in it's entirety can be found [here](https://github.com/petethegreat/HadoopSpark/blob/master/pyspark_Titanic.py), sections of it are described below.

## Data Cleaning/Transformation

First, the data is read in through spark. A validation set is partitioned off from the training data. This will be used to validate the model (in addtion to cross-validation on the training set), and may be used later to train an ensemble learner.

```{python,eval=FALSE}
    data = spark.read \
        .format("csv") \
        .option("header", "true") \
        .option('inferSchema','true') \
        .load(titanicTrainFile)

    # parition into training and validation sets
    train,val = data.randomSplit([0.8,0.2],seed=1991)
```    
Will take a quick look at the first few rows of data, and the schema.


```{python,eval=FALSE}
    train.limit(10).show()
    train.printSchema()
```
```
+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+
|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|
+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+
|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|
|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|
|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|
|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|
|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|
|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|
|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|
|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|
|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|
|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|
+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+

root
 |-- PassengerId: integer (nullable = true)
 |-- Survived: integer (nullable = true)
 |-- Pclass: integer (nullable = true)
 |-- Name: string (nullable = true)
 |-- Sex: string (nullable = true)
 |-- Age: double (nullable = true)
 |-- SibSp: integer (nullable = true)
 |-- Parch: integer (nullable = true)
 |-- Ticket: string (nullable = true)
 |-- Fare: double (nullable = true)
 |-- Cabin: string (nullable = true)
 |-- Embarked: string (nullable = true)

```

Survived is the categorical variable that we wish to predict. `Pclass`, `Sex`, `Embarked` are also categorical, and should be treated appropriately. Cabin seems to contain a lot of missing values, so will be omitted for now. There are also some missing values in Age (and potentially Fare), so these will be imputed.

A pipeline will be used to process the data. Spark ML classifiers (and regressors) operate on a vector column of features. Categorical variables need to be encoded as "one hot" before they can be included in the feature vector. `Sex` and `Embarked` first need to be indexed, such that the string categories (male/female, C/Q/S) are transformed to integers (0/1/2). There are some missing values in `Embarked`, the indexer (embarkIndexer) keeps these values and treats them as a fourth category (so no assumptions are made about these records). Each of the encoder stages transforms the indexed variable (an integer) to a one hot vector: there is an element in the vector for each observed class, and the category is information is encoded based on which element is equal to 1 (the rest are 0). 

```{python,eval=FALSE}
    # set up transformation pipeline stages
    # index "Sex" column and encode as one hot
    sexIndexer = StringIndexer(
        inputCol='Sex',
        outputCol='sex_word')
    sexEncoder = OneHotEncoder(
        inputCol=sexIndexer.getOutputCol(),
        outputCol='sex_cat',
        dropLast=False)
    # encode "Pclass" as one hot (rather than int)
    classEncoder = OneHotEncoder(
        inputCol='Pclass',
        outputCol='pclass_cat',
        dropLast=False)
    #index "Embarked" and encode as one hot. Keep null values
    embarkIndexer = StringIndexer(
        inputCol='Embarked',
        outputCol='embarked_word',
        handleInvalid="keep")
    embarkEncoder = OneHotEncoder(
        inputCol=embarkIndexer.getOutputCol(),
        outputCol='embarked_cat',
        dropLast=False)
    # impute missing age values
    ageFareImputer = Imputer(
        inputCols=['Age','Fare'],
        outputCols=['age_i','fare_i'])

    # combine age, Parch, and SibSp into a single vector
    ASPFvecAssembler = VectorAssembler(
        inputCols=['age_i','Parch','SibSp','fare_i'],
        outputCol='ASPF_raw')
    # shift and scale ASP vector
    ASPFscaler = StandardScaler(inputCol=ASPFvecAssembler.getOutputCol(),
        outputCol='ASPF_scaled',
        withMean=True,
        withStd=True)

    # Create a feature vector
    # first generate a list of column names
    featureCols = [x.getOutputCol() for x in [
        sexEncoder,
        classEncoder,
        embarkEncoder,
        ASPFscaler,
        ]]

    # assemble into a vector
    featureVecAssembler = VectorAssembler(
        inputCols=featureCols,
        outputCol='features')

```

## Model training

For prediction, a logistic regression model will be used. 


```{python,eval=FALSE}
    # logistic regression
    # regularisation parameters will be tuned via cross validation
    lr = LogisticRegression(
        maxIter=10,
        family='binomial',
        # regParam=0.0,
        labelCol='Survived',
        featuresCol='features',
        probabilityCol='lr_surviveProb',
        predictionCol='lr_survivePred',
        rawPredictionCol='lr_rawPrediction')
```        
We are interested in binomial (0/1) classification, so this is passed as the family parameter.

Before training, the pipeline must be assembled. Each of the transformers listed above is used as a pipeline stage, as well as the logistic regression model. 

```{python, eval=FALSE}
    # Assemble all the components
    pipestages = [sexIndexer,
                  sexEncoder,
                  classEncoder,
                  embarkIndexer,
                  embarkEncoder,
                  ageFareImputer,
                  ASPFvecAssembler,
                  ASPFscaler,
                  featureVecAssembler,
                  lr
                  ]
    # construct a pipeline
    pipeline = Pipeline(stages=pipestages)
```

The pipeline encompasses all of the steps that need to be applied to our raw data in order to get a desired result (in this case, a prediction). Some of these pipeline stages are estimators, in that they need to be fit to input data in order for them to learn how to carry out a transformation. The logistic regression model is an obvious example of this, but the indexers, imputer, and scaler are also estimators. For example, sexIndexer needs to see the column of male/female strings before learning to assign 0/1 values accordingly. Fitting the pipeline to the training data (```pipeline.fit(train)```) would pass the data through each stage of the pipeline. Transformer stages would act on the data and estimator stages would be fit, producing trained models that are transformer stages. The ```pipeline.fit()``` method returns a pipelineModel, which consists of all the transformation stages from the original pipeline, with all the estimator stages replaced by trained models. The resulting pipelineModel contains only transformer stages, such that it is ready to act on new data.

Instead of fitting the pipeline at this point, cross-validation will be used to determine the best parameters for the models. A paramGrid is constructed which describes the parameters that are to be tuned, and the values that should be considered. In this case, we're really only interested tuning the logistic regression stage. 

```{python,eval=FALSE}

    # specify parameter grid for cv tuning
    paramGrid = ParamGridBuilder() \
        .addGrid(lr.regParam,[0.0,0.01,0.1]) \
        .addGrid(lr.elasticNetParam, [0.0, 1.0]) \
        .build()
```
The regularisation parameter determines how much high value weights are penalised when fitting the model. The elastic net parameter determines how much of the regularisation is applied to the L1 or L2 norm of the weights (1.0 corresponds to lasso regression, while 0.0 corresponds to ridge regression). High value weights can yield models with large variance.

A BinaryClassificationEvaluator is used to evaluate hyperparameter settings, and determine the best tune. By default, the metric used is the area under the receiver operating characteristic (ROC AUC). Three-fold cross-validation will be used. The training data is split into three folds, and the model is trained on two and evaluate against the third. This is done three times (once for each test fold), and the model performance (the ROC AUC evaluation) is averaged. The parameters that give the best average results are then used to train the model on the entire training data set.

```{python,eval=FALSE}
    # initialise an evaluator. This uses ROC AUC as a metric by default.
    BCeval = BinaryClassificationEvaluator(labelCol='Survived',rawPredictionCol=lr.getRawPredictionCol())

    # initialise the cross validator object
    # will use 3 fold cross validation
    cv = CrossValidator(estimator=pipeline,estimatorParamMaps=paramGrid,evaluator=BCeval,numFolds=3,seed=1992)

    # Carry out the cross validation and obtain the model with the best performance
    cvModel = cv.fit(train)
    # print some performance metrics (these are averaged over the three folds)
    print('CrossValidation average metrics:')
    print(cvModel.avgMetrics)

    trained = cvModel.transform(train)
    trainedEval = BCeval.evaluate(trained)
    print('best model evaluation on training set = {v:0.4g}'.format(v=trainedEval))

    validation = cvModel.transform(val)
    # evaluate model on validation set
    valEval = BCeval.evaluate(validation)
```
In this case, our model had a ROC AUC of 0.8509 when applied to the training set, and 0.8777 on the validation set (i.e., data that the model has not been trained on). It should perform reasonably well on the test data. 

The ROC curve can be obtained from a BinaryLogisticRegressionSummary, which is obtained by evaluating the LogisticRegressionModel on our validation set. 
```{python, eval=FALSE}
        # get the summary/evaluation from the validation set
        # get the last stage of the pipe from the best model found in cv
        # evaluate returns a BinaryLogisticRegressionSummary
        # note that validation has already been transformed, we'll only keep the columns needed for evaluation

        summary = cvModel.bestModel.stages[-1].evaluate(validation.select(['features','Survived']))

        # get the roc data
        rocData = summary.roc
        rocData.coalesce(1).write.csv(path=rocOutput,header=True,mode='overwrite')
```

This ROC curve is plotted below
```{python}
import pandas as pd
import matplotlib.pyplot as plt

roc_data = pd.read_csv(titanic_lr_ROC.csv)
plt.plot(roc_data['FPR'],roc_data['TPR'])
plt.show()

```


The best model (specifically, the entire pipeline formed using the best model) is written to disk, such that it can be retrieved later without needing to be retrained.

```{python,eval=FALSE}
    print('best model evaluation on validation set = {v:0.4g}'.format(v=valEval))
    cvModel.bestModel.write().overwrite().save(modelDest)
```

Finally, the model will predict on the test set. The predictions are then saved (and submitted to Kaggle). The model scores 77.5% on the public leaderboard. 
```{python,eval=FALSE}
    test = spark.read \
        .format("csv") \
        .option("header", "true") \
        .option('inferSchema','true') \
        .load(titanicTestFile) 

    # load the saved model
    loadedModel = PipelineModel.load(modelFileName)
    # transform (predict on) the test data
    predicted = loadedModel.transform(test)
    # write predictions to file
    predicted.select(['PassengerId',predicted[modelPredColName].cast('int').alias('Survived')]) \
        .coalesce(1) \
        .write.csv(path=predictionFile,header=True,mode='overwrite')
    print('wrote test predictions to {fd}'.format(fd=predictionFile))
```


