---
title: "Pyspark ML pipeline"
output:
  html_document:
    theme: "cerulean" 
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  pdf_document:
    toc: true
---


## Introduction
This summarises some work I did using PySpark's machine learning (ML) library to predict survival for the Kaggle Titanic dataset. The data is loaded from a .csv file on hdfs into a spark dataframe, and then run through a spark pipeline that transforms the data and fits a logistic regression model. The output model is then applied to the test set. The Python file in it's entirety can be found [here](https://github.com/petethegreat/HadoopSpark/blob/master/pyspark_Titanic.py), sections of it are described below.

## Data Cleaning/Transformation

First, the data is read in through spark. A validation set is partitioned off from the training data. This will be used to validate the model (in addtion to cross-validation on the training set), and may be used later to train an ensemble learner.

```{Python,eval=FALSE}
    data = spark.read \
        .format("csv") \
        .option("header", "true") \
        .option('inferSchema','true') \
        .load(titanicTrainFile)

    # parition into training and validation sets
    train,val = data.randomSplit([0.8,0.2],seed=1991)
```    
Will take a quick look at the first few rows of data, and the schema.


```{Python,eval=FALSE}
    train.limit(10).show()
    train.printSchema()
```
```
+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+
|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|
+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+
|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|
|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|
|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|
|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|
|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|
|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|
|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|
|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|
|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|
|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|
+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+

root
 |-- PassengerId: integer (nullable = true)
 |-- Survived: integer (nullable = true)
 |-- Pclass: integer (nullable = true)
 |-- Name: string (nullable = true)
 |-- Sex: string (nullable = true)
 |-- Age: double (nullable = true)
 |-- SibSp: integer (nullable = true)
 |-- Parch: integer (nullable = true)
 |-- Ticket: string (nullable = true)
 |-- Fare: double (nullable = true)
 |-- Cabin: string (nullable = true)
 |-- Embarked: string (nullable = true)

```

Survived is the categorical variable that we wish to predict. `Pclass`, `Sex`, `Embarked` are also categorical, and should be treated appropriately. Cabin seems to contain a lot of missing values, so will be omitted for now. There are also some missing values in Age (and potentially Fare), so these will be imputed.

A pipeline will be used to process the data. Spark ML classifiers (and regressors) operate on a vector column of features. Categorical variables need to be encoded as "one hot" before they can be included in the feature vector. `Sex` and `Embarked` first need to be indexed, such that the string categories (male/female, C/Q/S) are transformed to integers (0/1/2). There are some missing values in `Embarked`, the indexer (embarkIndexer) keeps these values and treats them as a fourth category (so no assumptions are made about these records). Each of the encoder stages transforms the indexed variable (an integer) to a one hot vector: there is an element in the vector for each observed class, and the category is information is encoded based on which element is equal to 1 (the rest are 0). 

```
    # set up transformation pipeline stages
    # index "Sex" column and encode as one hot
    sexIndexer = StringIndexer(
        inputCol='Sex',
        outputCol='sex_word')
    sexEncoder = OneHotEncoder(
        inputCol=sexIndexer.getOutputCol(),
        outputCol='sex_cat',
        dropLast=False)
    # encode "Pclass" as one hot (rather than int)
    classEncoder = OneHotEncoder(
        inputCol='Pclass',
        outputCol='pclass_cat',
        dropLast=False)
    #index "Embarked" and encode as one hot. Keep null values
    embarkIndexer = StringIndexer(
        inputCol='Embarked',
        outputCol='embarked_word',
        handleInvalid="keep")
    embarkEncoder = OneHotEncoder(
        inputCol=embarkIndexer.getOutputCol(),
        outputCol='embarked_cat',
        dropLast=False)
    # impute missing age values
    ageFareImputer = Imputer(
        inputCols=['Age','Fare'],
        outputCols=['age_i','fare_i'])

    # combine age, Parch, and SibSp into a single vector
    ASPFvecAssembler = VectorAssembler(
        inputCols=['age_i','Parch','SibSp','fare_i'],
        outputCol='ASPF_raw')
    # shift and scale ASP vector
    ASPFscaler = StandardScaler(inputCol=ASPFvecAssembler.getOutputCol(),
        outputCol='ASPF_scaled',
        withMean=True,
        withStd=True)

    # Create a feature vector
    # first generate a list of column names
    featureCols = [x.getOutputCol() for x in [
        sexEncoder,
        classEncoder,
        embarkEncoder,
        ASPFscaler,
        ]]

    # assemble into a vector
    featureVecAssembler = VectorAssembler(
        inputCols=featureCols,
        outputCol='features')

```

For prediction, a logistic regression model will be used. 
```{python,eval=FALSE}
    # logistic regression
    # regularisation parameters will be tuned via cross validation
    lr = LogisticRegression(
        maxIter=10,
        family='binomial',
        # regParam=0.0,
        labelCol='Survived',
        featuresCol='features',
        probabilityCol='lr_surviveProb',
        predictionCol='lr_survivePred',
        rawPredictionCol='lr_rawPrediction')
```        
